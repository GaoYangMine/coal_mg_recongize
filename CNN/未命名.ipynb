{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-4c5de0ec3284>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-4c5de0ec3284>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    eight_loss(shape, stddev, wl):\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "eight_loss(shape, stddev, wl):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev= stddev))\n",
    "    if wl is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var), wl, name = \"weight_loss\")\n",
    "        tf.add_to_collection(\"losses\",weight_loss)\n",
    "    return var\n",
    "\n",
    "def BN(x, is_training , BN_decay = 0.9, BN_EPSILON = 1e-05):\n",
    "    x_shape = x.get_shape()\n",
    "    params_shapes = x_shape[-1:]\n",
    "\n",
    "    axis = list(range(len(x_shape)-1))\n",
    "\n",
    "    beta = tf.Variable(tf.zeros(shape=params_shapes))\n",
    "    gamma = tf.Variable(tf.ones(shape=params_shapes))\n",
    "\n",
    "    moving_mean = tf.Variable(tf.zeros(shape=params_shapes), name=\"mean\",trainable=False)\n",
    "    moving_variance = tf.Variable(tf.ones(shape=params_shapes), name = \"variance\",trainable=False)\n",
    "\n",
    "    mean, variance = tf.nn.moments(x, axis)\n",
    "\n",
    "    update_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, BN_decay)\n",
    "    update_moving_variance = moving_averages.assign_moving_average(moving_variance, variance, BN_decay)\n",
    "\n",
    "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_moving_mean)\n",
    "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_moving_variance)\n",
    "\n",
    "    mean, variance = control_flow_ops.cond(is_training, lambda:(mean, variance), lambda :(moving_mean, moving_variance))\n",
    "\n",
    "    return tf.nn.batch_normalization(x, mean, variance, beta, gamma, BN_EPSILON)\n",
    "\n",
    "\n",
    "# 卷积神经网络 卷积部分\n",
    "kernel1 = variable_with_weight_loss(shape=[3,3,3,32], stddev=0.05, wl = 0.0)\n",
    "conv1 = tf.nn.conv2d(x, kernel1, [1,1,1,1], padding=\"SAME\")\n",
    "bias1 = tf.Variable(tf.constant(0.0, shape=[32]))\n",
    "bn1 = BN(tf.nn.bias_add(conv1, bias1), is_training = is_training)\n",
    "relu1 = tf.nn.relu(bn1)\n",
    "# relu1 = tf.nn.relu(tf.nn.bias_add(conv1, bias1))\n",
    "pool1 = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,1,1,1],padding=\"SAME\")\n",
    "\n",
    "kernel2 = variable_with_weight_loss(shape=[3,3,32,32], stddev=0.05, wl = 0.0)\n",
    "conv2 = tf.nn.conv2d(pool1, kernel2, [1,1,1,1], padding=\"SAME\")\n",
    "bias2 = tf.Variable(tf.constant(0.1, shape=[32]))\n",
    "bn2 = BN(tf.nn.bias_add(conv2, bias2), is_training= is_training)\n",
    "relu2 = tf.nn.relu(bn2)\n",
    "# relu2 = tf.nn.relu(tf.nn.bias_add(conv2, bias2))\n",
    "pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,1,1,1],padding=\"SAME\")\n",
    "\n",
    "kernel3 = variable_with_weight_loss(shape=[3,3,32,64], stddev=0.05, wl = 0.0)\n",
    "conv3 = tf.nn.conv2d(pool2, kernel3, [1,1,1,1], padding=\"SAME\")\n",
    "bias3 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "bn3 = BN(tf.nn.bias_add(conv3, bias3), is_training = is_training)\n",
    "relu3 = tf.nn.relu(bn3)\n",
    "# relu3 = tf.nn.relu(tf.nn.bias_add(conv3, bias3))\n",
    "pool3 = tf.nn.max_pool(relu3, ksize=[1,3,3,1], strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "kernel4 = variable_with_weight_loss(shape=[3,3,64,64], stddev=0.05, wl = 0.0)\n",
    "conv4 = tf.nn.conv2d(pool3, kernel4, [1,1,1,1], padding=\"SAME\")\n",
    "bias4 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "bn4 = BN(tf.nn.bias_add(conv4, bias4), is_training=is_training)\n",
    "relu4 = tf.nn.relu(bn4)\n",
    "# relu4 = tf.nn.relu(tf.nn.bias_add(conv4, bias4))\n",
    "pool4 = tf.nn.max_pool(relu4, ksize=[1,3,3,1], strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "reshape = tf.reshape(pool4, [batch_size, -1])\n",
    "dim = reshape.get_shape()[1].value\n",
    "\n",
    "# 全相连神经网络部分\n",
    "weight1 = variable_with_weight_loss([dim, 128], stddev=0.04, wl = 0.004)\n",
    "fc_bias1 = tf.Variable(tf.constant(0.1, shape = [128]))\n",
    "# fc_1 = tf.nn.relu(tf.matmul(reshape, weight1) + fc_bias1)\n",
    "\n",
    "weight2 = variable_with_weight_loss([128, 64], stddev=0.04, wl = 0.004)\n",
    "fc_bias2 = tf.Variable(tf.constant(0.1, shape = [64]))\n",
    "# fc_2 = tf.nn.relu(tf.matmul(fc_1, weight2) + fc_bias2)\n",
    "\n",
    "weight3 = variable_with_weight_loss([64,2], stddev=1/64, wl = 0.0)\n",
    "fc_bias3 = tf.Variable(tf.constant(0.1, shape=[2]))\n",
    "# result = tf.add(tf.matmul(fc_2, weight3), fc_bias3)\n",
    "\n",
    "dro = tf.placeholder(tf.float32)\n",
    "\n",
    "def hidden_layer(input_tensor, w1,b1, w2,b2, w3,b3, dro,layer_name):\n",
    "    fc1 = tf.nn.relu(tf.matmul(input_tensor,w1) + b1)\n",
    "    fc_1_droped = tf.nn.dropout(fc1, dro)\n",
    "\n",
    "    fc2 = tf.nn.relu(tf.matmul(fc_1_droped, w2) + b2)\n",
    "    fc_2_droped = tf.nn.dropout(fc2, dro)\n",
    "\n",
    "    return tf.add(tf.matmul(fc_2_droped, w3), b3)\n",
    "\n",
    "#全相连网络部分\n",
    "result = hidden_layer(reshape, weight1,fc_bias1,weight2,fc_bias2,\n",
    "                      weight3,fc_bias3,dro = dro,layer_name=\"y\")\n",
    "\n",
    "\n",
    "# 滑动平均值部分\n",
    "# averages_class = tf.train.ExponentialMovingAverage(0.99, training_step)\n",
    "# averages_op = averages_class.apply(tf.trainable_variables())\n",
    "# averages_y = hidden_layer(reshape, averages_class.average(weight1),averages_class.average(fc_bias1),\n",
    "#                           averages_class.average(weight2),averages_class.average(fc_bias2),\n",
    "#                           averages_class.average(weight3),averages_class.average(fc_bias3),\n",
    "#                           layer_name=\"average_y\")\n",
    "\n",
    "# 损失值\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=result,\n",
    "                                                               labels=tf.cast(y_, tf.int32))\n",
    "learn_rate = tf.train.exponential_decay(learning_rate, training_step, num_train/batch_size*3,\n",
    "                                        learning_decay_rate)\n",
    "\n",
    "weight_with_l2_loss = tf.add_n(tf.get_collection(\"losses\"))\n",
    "loss = tf.reduce_mean(cross_entropy) + weight_with_l2_loss\n",
    "# loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# train_step = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss, global_step=training_step)\n",
    "# train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "update_op = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_op):\n",
    "    train_step = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss, global_step=training_step)\n",
    "\n",
    "\n",
    "def train():\n",
    "    train_accuracys = []\n",
    "    test_accuracys = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        # tf.train.start_queue_runners()\n",
    "\n",
    "        circle = int(num_train/batch_size)\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            start_time = time.time()\n",
    "            i = int(step%circle)\n",
    "\n",
    "            files = train_files[i*batch_size: i*batch_size + batch_size]\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for file in files:\n",
    "                img = cv2.imread(\"data_for_cnn/train/\"+file)\n",
    "                label = int(file[0])\n",
    "                x_batch.append(img)\n",
    "                y_batch.append(label)\n",
    "            x_batch = np.array(x_batch, dtype=np.float32)\n",
    "            y_batch = np.array(y_batch, dtype=np.uint8)\n",
    "\n",
    "            print(step,i*batch_size, i*batch_size + batch_size)\n",
    "            # sess.run(train_op, feed_dict={x:x_batch, y_:y_batch})\n",
    "            sess.run(train_step, feed_dict={x:x_batch, y_:y_batch, dro:1, is_training:True})\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "\n",
    "            if step%10 == 0:\n",
    "\n",
    "                gc.collect()\n",
    "\n",
    "                examples_per_sec = batch_size/duration\n",
    "                sec_per_batch = float(duration)\n",
    "\n",
    "                true_count = 0\n",
    "                total_test_num = 32*batch_size\n",
    "\n",
    "                x_train_batches , y_train_batches = random_batches(32, \"train\")\n",
    "\n",
    "                for x_batch,y_batch in zip(x_train_batches,y_train_batches):\n",
    "                    predicition,train_loss = sess.run([result,loss], feed_dict={x: x_batch, y_: y_batch, dro:1, is_training:False})\n",
    "                    predicition = np.argmax(predicition, axis=1)\n",
    "                    predicition = predicition == y_batch.astype(\"int32\")\n",
    "                    true_count += np.sum(predicition.astype(\"int32\"))\n",
    "\n",
    "                print(true_count,total_test_num)\n",
    "                train_acc = true_count / total_test_num\n",
    "                train_accuracys.append(train_acc)\n",
    "                train_losses.append(train_loss)\n",
    "                true_count = 0\n",
    "                total_test_num = 16*batch_size\n",
    "\n",
    "                x_test_batches, y_test_batches = random_batches(16, \"test\")\n",
    "\n",
    "                for x_batch,y_batch in zip(x_test_batches,y_test_batches):\n",
    "                    predicition, test_loss = sess.run([result, loss], feed_dict={x: x_batch, y_: y_batch, dro:1, is_training:False})\n",
    "                    predicition = np.argmax(predicition, axis=1)\n",
    "                    predicition = predicition == y_batch.astype(\"int32\")\n",
    "                    true_count += np.sum(predicition.astype(\"int32\"))\n",
    "\n",
    "                test_acc = true_count/total_test_num\n",
    "                test_accuracys.append(test_acc)\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "                print(true_count,total_test_num)\n",
    "                print(\"step %d,train accuracy = %.2f, test accuracy = %.2f (%.1f examples/sec; %.3f sec/batch)\" %\n",
    "                      (step, train_acc*100, test_acc*100 , examples_per_sec, sec_per_batch))\n",
    "        saver.save(sess, \"model/cnn.ckpt\")\n",
    "\n",
    "    return train_accuracys, test_accuracys, train_losses, test_losses\n",
    "\n",
    "def random_batches(size,type):\n",
    "    if type == \"train\":\n",
    "        files = train_files\n",
    "        indices = np.random.choice(range(num_train), size * batch_size)\n",
    "    else:\n",
    "        files = test_files\n",
    "        indices = np.random.choice(range(num_test), size * batch_size)\n",
    "\n",
    "    # print(type, indices)\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(size):\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for j in indices[i * batch_size:i * batch_size + batch_size]:\n",
    "            file = files[j]\n",
    "            img = cv2.imread(\"data_for_cnn/\"+type+\"/\" + file)\n",
    "            label = int(file[0])\n",
    "            x_batch.append(img)\n",
    "            y_batch.append(label)\n",
    "\n",
    "        x_batch = np.array(x_batch, dtype=np.float32)\n",
    "        y_batch = np.array(y_batch, dtype=np.uint8)\n",
    "        # print(x_batch.shape, y_batch.shape)\n",
    "\n",
    "        x_batches.append(x_batch)\n",
    "        y_batches.append(y_batch)\n",
    "\n",
    "    return x_batches,y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
